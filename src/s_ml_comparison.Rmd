---
title: "ML model comparison"
output: html_notebook
---

```{r}
rm(list=ls())
library(tidyverse)
library(reshape2)
library(rstan)
library(latex2exp)
library(caret)
library(gbm)
options(mc.cores=4)
rstan_options(auto_write = TRUE)
source("monitornew.R")
source("r_star_monitor.R")
```

# AR1 model
```{r}
f_ar1 <- function(rho, sigma, L){
  x <- vector(length = L)
  x[1] <- rnorm(1, 0, sd=sigma)
  for(i in 2:L)
    x[i] = rho * x[i - 1] + rnorm(1, 0, sd=sigma)
  return(x)
}

# Generates three chains with same var; one with a different var
f_generate_lower_var_four <- function(var_ratio, rho, sigma, L){
  x <- matrix(nrow = L, ncol = 4)
  for(i in 1:3)
    x[, i] <- f_ar1(rho, sigma, L)
  z <- f_ar1(rho, sigma * sqrt(var_ratio), L)
  x[, 4] <- z
  return(x)
}

r_star_ml_variety <- function(x, split_chains=T, training_percent=0.7, method_="gbm", caretGrid=NULL){
  
  if(split_chains)
    x <- split_data(x)
  
  nparams <- dim(x)[3]
  nchains <- dim(x)[2]
  niter <- dim(x)[1]
  m_flattened <- matrix(nrow = nrow(x) * nchains,
                        ncol = (nparams + 1))
  k <- 1
  for(i in 1:nchains){
    for(j in 1:nrow(x)){
      m_flattened[k, 1:nparams] <- x[j, i, ]
      m_flattened[k, (nparams + 1)] <- i
      k <- k + 1
    }
  }
  m_flattened <- m_flattened %>% 
    as.data.frame()
  colnames(m_flattened)[nparams + 1] <- "chain"
  r <- m_flattened %>% 
    mutate(chain=as.factor(chain))
  # if only 1 param, add in a column of random noise since gbm requires >1 dims
  if(nparams==1)
    r <- r %>% 
      mutate(V_new=rnorm(nrow(r)))
  
  rand_samples <- sample(1:nrow(r), training_percent * nrow(r))
  training_data <- r[rand_samples, ]
  testing_data <- r[-rand_samples, ]
  
  if(!is.null(caretGrid))
    gbmFit <- train(chain ~ ., data = training_data, 
                   trControl = trainControl(method = 'none'),
                   method=method_,
                   tuneGrid = caretGrid)
  else
    gbmFit <- train(chain ~ ., data = training_data, 
                   trControl = trainControl(method = 'none'),
                   method=method_)
  plda <- predict(object=gbmFit, newdata=testing_data)
  a_accuracy <- 
    tibble(predicted=plda, actual=testing_data$chain) %>%
    mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
    summarise(mean(correct)) %>% 
    pull()
  return(a_accuracy * n_distinct(training_data$chain))
}

f_replicate_ml_variety <- function(x, method_, caretGrid_=NULL){
  temp <- x
  a_array <- array(dim=c(nrow(x), 4, 1))
  a_array[,,1] <- temp
  a_accuracy <- r_star_ml_variety(a_array, method_=method_, caretGrid = caretGrid_)
  return(a_accuracy)
}

f_replicate_rhat <- function(x){
  temp <- x
  a_array <- array(dim=c(2000, 4, 1))
  a_array[,,1] <- temp
  mon <- monitor_extra(split_data(a_array))
  a_accuracy <- mon$zfsRhat
  return(a_accuracy)
}

f_replicate_ml_variety_gridded <- function(x, method_="gbm", caretGrids_){
  n_things <- nrow(caretGrids_)
  v_accuracy <- length(n_things)
  for(i in 1:n_things){
    caretGrid <- caretGrids_[i, ]
    v_accuracy[i] <- f_replicate_ml_variety(x, method_, caretGrid)
  }
  return(list(a_accuracy=max(v_accuracy), hyper=caretGrids_[which.max(v_accuracy),]))
}

tunegrid_rf <- tibble(mtry = 1:2)
tunegrid_gbm <- expand.grid(interaction.depth=c(3), 
                             n.trees = 50,
                             shrinkage=c(0.1),
                             n.minobsinnode=10)
tunegrid_knn <- tibble(k = c(5, 10, 15, 20, 40))
tunegrid_svm <- tibble(C = c(0.25, 0.5, 0.75))
tunegrid_nnet <- tibble(size=c(2, 3, 4, 5), decay=c(0.5, 0.5, 0.5, 0.5))
tunegrid_multinom <- tibble(decay=c(0.1, 0.2, 0.5, 1))
niterates <- 50
m_res <- matrix(nrow=niterates, ncol = 7)
l_rf_hyper <- vector(length = niterates, mode="list")
l_knn_hyper <- vector(length = niterates, mode="list")
l_svm_hyper <- vector(length = niterates, mode="list")
l_nnet_hyper <- vector(length = niterates, mode="list")
l_multinom_hyper <- vector(length = niterates, mode="list")
for(i in 1:niterates){
  print(i)
  x <- f_generate_lower_var_four(1/3, 0.3, 1, 2000)
  m_res[i, 1] <- f_replicate_ml_variety(x, "gbm", tunegrid_gbm)
  m_res[i, 2] <- f_replicate_rhat(x)
  temp <- f_replicate_ml_variety_gridded(x, method_="rf", caretGrids_ = tunegrid_rf)
  l_rf_hyper[[i]] <- temp$hyper
  m_res[i, 3] <- temp$a_accuracy
  temp <- f_replicate_ml_variety_gridded(x, "knn", tunegrid_knn)
  l_knn_hyper[[i]] <- temp$hyper
  m_res[i, 4] <- temp$a_accuracy
  temp <- f_replicate_ml_variety_gridded(x, "svmLinear", tunegrid_svm)
  l_svm_hyper[[i]] <- temp$hyper
  m_res[i, 5] <- temp$a_accuracy
  temp <- f_replicate_ml_variety_gridded(x, "nnet", tunegrid_nnet)
  l_nnet_hyper[[i]] <- temp$hyper
  m_res[i, 6] <- temp$a_accuracy
  temp <- f_replicate_ml_variety_gridded(x, "multinom", tunegrid_multinom)
  l_nnet_hyper[[i]] <- temp$hyper
  m_res[i, 7] <- temp$a_accuracy
}

colnames(m_res) <- c("gbm", "rhat", "rf", "knn", "svm", "nnet")
m_res1 <- m_res %>%
  as.data.frame() %>% 
  mutate(iter=seq_along(gbm))
saveRDS(m_res1, "../data/ml_comparison.rds")
m_res1 <- readRDS("../data/ml_comparison.rds")
g <- m_res1 %>% 
  dplyr::select(-one_of("rhat")) %>% 
  melt(id.vars="iter") %>% 
  ggplot(aes(x=fct_reorder(as.factor(variable), value), y=value)) +
  geom_boxplot() +
  xlab("ML model") +
  ylab("R*") +
  theme(text=element_text(colour="black", size=14))
ggsave("../output/ml_comparison_ar1.pdf", g, width = 12, height = 8)
```

# Eight schools model
```{r}
source("eight_schools.data.R")
eight_schools <- list(J=J, y=y, sigma=sigma)
model_cp <- stan_model("eight_schools_cp.stan")
model_ncp <- stan_model("eight_schools_ncp.stan")

fit_cp <- sampling(
  model_cp, data = eight_schools,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)

fit_ncp <- sampling(
  model_ncp, data = eight_schools,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)

x_cp <- rstan::extract(fit_cp, permuted=F)
x_ncp <- rstan::extract(fit_ncp, permuted=F)
```

Try out different ML models
```{r}
f_replicate_ml_simple <- function(x, method_, caretGrid_=NULL){
  a_array <- x
  a_accuracy <- r_star_ml_variety(a_array, method_=method_, caretGrid = caretGrid_)
  return(a_accuracy)
}

f_replicate_rhat_simple <- function(x){
  a_array <- x
  mon <- monitor_extra(split_data(a_array))
  a_accuracy <- mon$zfsRhat
  return(a_accuracy)
}

f_replicate_ml_simple_gridded <- function(x, method_="gbm", caretGrids_){
  n_things <- nrow(caretGrids_)
  v_accuracy <- length(n_things)
  for(i in 1:n_things){
    caretGrid <- caretGrids_[i, ]
    v_accuracy[i] <- f_replicate_ml_simple(x, method_, caretGrid)
  }
  return(list(a_accuracy=max(v_accuracy), hyper=caretGrids_[which.max(v_accuracy),]))
}

m_res <- matrix(nrow=2, ncol = 6)
l_rf_hyper <- vector(length = 2, mode="list")
l_knn_hyper <- vector(length = 2, mode="list")
l_svm_hyper <- vector(length = 2, mode="list")
l_nnet_hyper <- vector(length = 2, mode="list")
x <- x_cp
i <- 1
m_res[i, 1] <- f_replicate_ml_simple(x, "gbm", tunegrid_gbm)
m_res[i, 2] <- max(f_replicate_rhat_simple(x))
temp <- f_replicate_ml_simple_gridded(x, method_="rf", caretGrids_ = tunegrid_rf)
l_rf_hyper[[i]] <- temp$hyper
m_res[i, 3] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "knn", tunegrid_knn)
l_knn_hyper[[i]] <- temp$hyper
m_res[i, 4] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "svmLinear", tunegrid_svm)
l_svm_hyper[[i]] <- temp$hyper
m_res[i, 5] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "nnet", tunegrid_nnet)
l_nnet_hyper[[i]] <- temp$hyper
m_res[i, 6] <- temp$a_accuracy
x <- x_ncp
i <- 2
m_res[i, 1] <- f_replicate_ml_simple(x, "gbm", tunegrid_gbm)
m_res[i, 2] <- max(f_replicate_rhat_simple(x))
temp <- f_replicate_ml_simple_gridded(x, method_="rf", caretGrids_ = tunegrid_rf)
l_rf_hyper[[i]] <- temp$hyper
m_res[i, 3] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "knn", tunegrid_knn)
l_knn_hyper[[i]] <- temp$hyper
m_res[i, 4] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "svmLinear", tunegrid_svm)
l_svm_hyper[[i]] <- temp$hyper
m_res[i, 5] <- temp$a_accuracy
temp <- f_replicate_ml_simple_gridded(x, "nnet", tunegrid_nnet)
l_nnet_hyper[[i]] <- temp$hyper
m_res[i, 6] <- temp$a_accuracy

colnames(m_res) <- c("gbm", "rhat", "rf", "knn", "svm", "nnet")
m_res <- m_res %>%
  as.data.frame() %>% 
  mutate(model=c("cp", "ncp"))
# saveRDS(m_res, "../data/ml_comparison_8_schools.rds")
m_res %>% 
  dplyr::select(-rhat) %>% 
  melt(id.vars="model") %>% 
  ggplot(aes(x=fct_reorder(as.factor(variable), value), y=value, colour=as.factor(model))) +
  geom_point(size=4) +
  xlab("ML model") +
  scale_color_brewer("8 schools model", palette = "Dark2") +
  ylab("R*") +
  theme(text=element_text(colour="black", size=14))
# ggsave("../output/ml_comparison_8_schools.pdf", width = 12, height = 8)

x_ncp[,,11] %>%
  as.data.frame() %>%
  mutate(iter=seq(1,2000,1)) %>%
  melt(id.vars="iter") %>%
  ggplot(aes(x=value, fill=as.factor(variable))) +
  stat_density(position="identity", alpha=0.2)
```

Looking into rf performance on 8 schools model more closely
```{r}
x <- x_cp
x <- split_data(x)
  
nparams <- dim(x)[3]
nchains <- dim(x)[2]
niter <- dim(x)[1]
m_flattened <- matrix(nrow = nrow(x) * nchains,
                      ncol = (nparams + 1))
k <- 1
for(i in 1:nchains){
  for(j in 1:nrow(x)){
    m_flattened[k, 1:nparams] <- x[j, i, ]
    m_flattened[k, (nparams + 1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame()
colnames(m_flattened)[nparams + 1] <- "chain"
r <- m_flattened %>% 
  mutate(chain=as.factor(chain))
  
rand_samples <- sample(1:nrow(r), 0.7 * nrow(r))
training_data <- r[rand_samples, ]
testing_data <- r[-rand_samples, ]
gbmFit <- train(chain ~ ., data = training_data, 
               trControl = trainControl(method = 'none'),
               method="rf",
               tuneGrid = tibble(mtry=1))
plda <- predict(object=gbmFit, newdata=testing_data)
a_accuracy <- 
  tibble(predicted=plda, actual=testing_data$chain) %>%
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct)) %>% 
  pull()
a_accuracy * n_distinct(training_data$chain)
varImp(gbmFit)

x[ , , 9] %>%
  as.data.frame() %>%
  mutate(iter=seq(1,1000,1)) %>%
  melt(id.vars="iter") %>%
  ggplot(aes(x=value, fill=as.factor(variable))) +
  stat_density(position="identity", alpha=0.2)

tibble(var1=testing_data$V1, var2=testing_data$V9, chain=plda) %>% 
  ggplot(aes(x=var1, y=var2, colour=as.factor(chain))) +
  geom_jitter()
```


Cauchy model
```{r}
fit_nom <- stan(file = 'cauchy_nom.stan', seed = 7878,
                refresh = 0)
x <- rstan::extract(fit_nom, permuted=F)
temp1 <- f_replicate_ml_simple(x, "gbm", tunegrid_gbm)
temp <- f_replicate_ml_simple_gridded(x, "rf", tibble(mtry=1))
temp$a_accuracy


fit_alt1 <- stan(file = 'cauchy_alt_1.stan',
                 seed = 7878, refresh = 0)
x <- rstan::extract(fit_alt1, permuted=F)
temp1 <- f_replicate_ml_simple(x, "gbm", tunegrid_gbm)
temp <- f_replicate_ml_simple_gridded(x, "rf", tibble(mtry=1))
temp$a_accuracy
```

AR1 vs independent
```{r}
f_generate_lower_var_four_independent <- function(var_ratio, rho, sigma, L){
  x <- matrix(nrow = L, ncol = 4)
  for(i in 1:3)
    x[, i] <- rnorm(L, 0, sqrt(sigma^2/(1-rho^2)))
  z <- rnorm(L, 0, sqrt(var_ratio * sigma^2/(1-rho^2)))
  x[, 4] <- z
  return(x)
}

rho <- 0.95
lgbm <- vector(length=2)
lrf <- vector(length=2)
x <- f_generate_lower_var_four(1/3, rho, 1, 10000)
var(x[, 4])
lgbm[1] <- f_replicate_ml_variety(x, "gbm", tunegrid_gbm)
lrf[1] <- f_replicate_ml_variety(x, "rf", tibble(mtry=1))

x <- f_generate_lower_var_four_independent(1/3, rho, 1, 10000)
var(x[, 4])
lgbm[2] <- f_replicate_ml_variety(x, "gbm", tunegrid_gbm)
lrf[2] <- f_replicate_ml_variety(x, "rf", tibble(mtry=1))
```

Try optimal Bayes classifier: two normals
```{r}
L <- 10000
x1 <- rnorm(L, 0, 1)
x2 <- rnorm(L, 1, 1)
x <- matrix(c(x1, x2), byrow = F, ncol=2)

a_array <- array(dim=c(nrow(x), 2, 1))
a_array[,,1] <- x
r_star_ml_variety(a_array, method_ ="rf", caretGrid = tibble(mtry=2))
```

Optimal classifier any dimension data
```{r}
# generate a correlation matrix via LKJ
rlkj <- function(n, nu, d){
  r_list <- vector(length = n, mode = 'list')
  for(i in 1:n){
    if(d==1)
      r = as.array(1)
    else if(d==2){
      rho <- 2 * rbeta(1, nu, nu) - 1
      r <- matrix(c(1, rho, rho, 1), ncol = 2)
    }else{
      beta <- nu + (d - 2) / 2
      u <- rbeta(1, beta, beta)
      r_12 <- 2 * u - 1
      r <- matrix(c(1, r_12, r_12, 1), ncol = 2)
      for(m in 2:(d - 1)){
        beta <- beta - 0.5
        y <- rbeta(1, m / 2, beta)
        a <- rnorm(m)
        anorm <- sqrt(sum(a^2))
        u <- a / anorm
        w <- sqrt(y) * u
        A <- chol(r)
        z <- w%*%A
        r <-cbind(r, t(z))
        r <- rbind(r, c(z, 1))
      }
    }
    r_list[[i]] <- r
  }
  return(r_list)
}

nd <- 50
df <- 200
Sigma1 <- rlkj(1, 50, nd)[[1]]
Sigma2 <- rlkj(1, 50, nd)[[1]]

# generate any dimensional data
library(mvtnorm)
n <- 2000
x1 <- rmvnorm(n, rep(0, nd), Sigma1)
x2 <- rmvnorm(n, rep(0, nd), Sigma2)

# calculate likelihood from each mode for each point
lp11 <- map_dbl(seq(1, nrow(x1), 1), ~dmvnorm(x1[., ], rep(0, nd), Sigma1, log = T))
lp12 <- map_dbl(seq(1, nrow(x1), 1), ~dmvnorm(x1[., ], rep(0, nd), Sigma2, log = T))
lp21 <- map_dbl(seq(1, nrow(x2), 1), ~dmvnorm(x2[., ], rep(0, nd), Sigma1, log = T))
lp22 <- map_dbl(seq(1, nrow(x2), 1), ~dmvnorm(x2[., ], rep(0, nd), Sigma2, log = T))
r_star_opt <- 0.5 * (mean(lp11>lp12) + mean(lp22>lp21)) / 0.5

# calculate R* from gbm and rf
x <- array(dim=c(nrow(x1), 2, ncol(x1)))
x[, 1, ] <- x1
x[, 2, ] <- x2
r_star(x, split_chains = T)

f_replicate_ml_variety_simple <- function(x, method_, caretGrid_=NULL){
  temp <- x
  a_accuracy <- r_star_ml_variety(x, method_=method_, caretGrid = caretGrid_)
  return(a_accuracy)
}
f_replicate_ml_variety_simple(x, "rf", tibble(mtry=20))
f_replicate_ml_variety_simple(x, "gbm",
                              tibble(interaction.depth=c(15), 
                              n.trees = 200,
                              shrinkage=c(0.1),
                              n.minobsinnode=40))
```

